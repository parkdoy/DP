# AI 기반 게임 매크로 프로그램 구현 계획

**이 문서는 게임 화면을 인식하고 캐릭터 간의 거리 변수에 따라 자동으로 버프를 제공하는 매크로 프로그램의 전체 구현 계획을 담고 있습니다. 매크로 방지 프로그램에 걸리지 않기 위해 커널 수준의 키보드 입력을 사용하는 것이 핵심 목표입니다.**

---
## 결과물
<img src="Result.gif" width="300" height="200"/>

---
## 과정
### 1단계: AI 기반 게임 화면 인식 및 객체 분류

* **목표:** AI를 활용해 게임 화면 속 내 캐릭터와 동료 캐릭터를 정확하게 인식하고 분류하는 것임.
* **기술:** **객체 탐지(Object Detection)** 기술이 필요해 실시간 탐지에 최적화된 **YOLO(You Only Look Once)** 모델을 사용했음.
---
#### 데이터 준비

* **스크린샷 수집:** 다양한 게임 상황(전투, 이동 등)에서 수백 장의 스크린샷을 수집했음. **`screenshots.py`**
    * 스크린샷 수집을 다시 진행 할때  파일이 덮어씌워지는 문제를 해결하기 위해 **`rename_files.py`** 스크립트를 사용했음.
* **데이터 라벨링:** 수작업으로 내 캐릭터(`my_character`)와 동료 캐릭터(`ally_character`) 영역에 바운딩 박스를 표시했음.
    * **LabelImg** 프로그램을 활용했으며, 라벨링 프로그램의 변수 형식 오류를 해결하는 과정이 있었음. [해결 참조](https://velog.io/@hglee_gun/Data-Labeling)
    * 학습용 이미지와 검증용 이미지 폴더 구조화 작업 중 **절대 경로**가 아닌 **상대 경로**로 데이터셋 경로를 설정하는 것이 중요했음.
    * **데이터 라벨링**과 **세그멘테이션**의 차이를 명확히 이해했음.
        |구분|	데이터 라벨링 (객체 탐지)  |세그멘테이션 (인스턴스)
        |:---|---:|:---:|
        |목표|	객체의 위치를 사각형으로 감지  |객체의 정확한 모양을 픽셀 단위로 분할
        |결과물|	(x, y, w, h) 형식의 사각형 좌표|객체 형태를 따라 픽셀 단위로 생성된 마스크
        |세밀함|	낮음 (객체의 대략적인 위치 파악) |매우 높음 (픽셀 단위의 정교한 경계)
        |사용 예시|	자율주행차에서 보행자 위치 파악, 게임 캐릭터 인식|의료 영상 분석(암세포 영역 분할), 배경 제거 앱
---
#### 학습 및 추론

* **모델 학습:** 수집된 데이터를 기반으로 YOLO 모델을 학습시켰음.
    * 100장의 데이터로는 인식률이 낮아 290장으로 학습을 진행했고, 실제 성능이 꽤 괜찮아졌음.
    * 수작업의 한계로 인해 더 많은 이미지를 사용하지 못했으며, **스테이블 디퓨전의 SAM**을 활용한 자동 라벨링을 고려했지만 포기했음.
* **프로그램 실행:** `screenshots.py`를 응용해 프로그램 실행 시 실시간으로 게임 화면을 캡처하고 객체 위치 좌표를 추출했음.

---

## 2단계: 버프 기능 구현 (거리 계산)

- **좌표 활용:** 화면 인식 단계에서 얻은 캐릭터 좌표를 이용하여 버프 제공 여부 결정
- **거리 계산:** 
  - 바운딩 박스 중심 좌표 간 거리 계산
  - 피타고라스 정리 활용:  
    `distance = sqrt((x2 - x1)^2 + (y2 - y1)^2)`
  - 픽셀 단위로 정확한 거리 계산

  - 버프 조건
    - 계산된 거리가 미리 정해진 임계값보다 작을 경우
    - 버프 재사용 대기 시간이 아닐 경우
---

## 3단계: 키보드 입력

- **목표:** 매크로 방지 프로그램 감지를 피하기 위해 OS의 API를 직접 호출
- **구현 상세:**
  - 파이썬에서 AI 및 게임 로직 수행
  - C++ 모듈로 커널 수준 키보드 입력 처리 (ctypes 또는 Pybind11 사용)

### 운영체제별 구현
- **Windows:** WinAPI `SendInput()` 함수 적용 실패로 pydirectinput를 사용

---

## 전체 프로그램 구조

```mermaid
graph TD
    A[게임 화면 캡처] --> B(딥러닝 모델로 이미지 분석)
    B --> C{내 캐릭터 & 동료 캐릭터 좌표}
    C --> D[거리 계산]
    D --> E{버프 조건 만족? (거리 < 임계값)}
    E -- Yes --> F[C++ 버프 함수 호출]
    F --> G[C++ 커널 수준 키보드 입력]
    G --> H[게임]
    E -- No --> I[대기 및 반복]
    I --> A
```
---

### 알게 된 핵심 개념들

1.  **딥러닝 모델의 기본 동작**
    * **반복적인 화면 캡처:** `capture_screen()` 함수가 `while True` 루프 안에서 매번 화면을 캡처했음.
    * **실시간 객체 인식:** 이렇게 캡처된 이미지가 **YOLO 모델**에 입력되어 실시간으로 게임 캐릭터를 탐지했음.
    * **결과 시각화:** `cv2.imshow()` 함수로 탐지된 객체의 위치에 바운딩 박스를 그려서 시각적으로 확인했음.

2.  **모델 학습 결과 해석** 
    <br><img src="yolo_project/Game_Macro/companion_try/results.png" width="600" height="400"/></br>
    * **손실(Loss) 하락:** `train_loss`와 `val_loss` 그래프에서 값이 지속적으로 하락하는 것은 모델이 학습 데이터의 오차를 잘 줄여가고 있음을 의미했음.
    * **평가지표(Metrics) 상승:** `precision`, `recall`, `mAP`와 같은 지표들이 상승하는 것은 모델의 성능이 향상되고 있음을 보여줬음.
    * **성공적인 학습:** 손실은 낮고 평가지표는 높은 이상적인 학습 결과가 나와, 모델이 게임 화면의 객체를 정확하게 인식할 준비가 되었음을 확인했음.

3.  **추가 학습(Fine-tuning)**
    * **기존 지식 활용:** 추가 학습은 처음부터 다시 학습하는 게 아니라, 이전에 학습된 `best.pt` 모델의 지식을 바탕으로 새로운 데이터를 학습시키는 방법임.
    * **모델 지정:** `yolo task=detect mode=train model=yolo_project/runs/detect/train/weights/best.pt`와 같이 기존 모델 경로를 지정해서 학습을 시작했음.

4.  **데이터 라벨링 효율화**
    * **마우스 드래그 캡처:** `get_region_from_drag()` 함수를 활용하여 전체 화면이 아닌 특정 영역만 캡처해서 필요한 데이터만 얻었음.
    * **스테이블 디퓨전 활용:** **세그먼트 기능**을 통해 객체를 자동으로 분리하여 파인튜닝에 필요한 바운딩 박스 데이터를 빠르게 만들 수 있었음.
    * **파일 이름 일괄 변경:** 파이썬 스크립트를 사용해 `screenshot_001.png`와 같은 파일 이름을 `companion_001.png`로 일괄 변경하여 데이터셋을 효율적으로 관리했음.

---
### 최소 기능으로 제작한 프로젝트로 간단한 테스트 외 실사용하지 않음.